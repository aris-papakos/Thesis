{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (RegexTokenizer, Tokenizer, HashingTF, IDF,\n",
    "                                StopWordsRemover, CountVectorizer, StopWordsRemover, StringIndexer, OneHotEncoder)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                  MulticlassClassificationEvaluator)\n",
    "from pyspark.sql.types import (LongType ,StringType, IntegerType,\n",
    "                               FloatType, DoubleType, ArrayType)\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train-Validation-Test DOI sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR ENSEMBLE EXPERIMENT ONLY!!!\n",
    "df_doiCateg = spark.sql(\"SELECT * FROM taxiarchis.doi_categories_4_4_features\")\n",
    "df = spark.sql(\"SELECT * FROM taxiarchis.section_4_4_ensemble\").dropna()\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Category\", outputCol=\"label\")\n",
    "indexed = indexer.fit(df_doiCateg).transform(df_doiCateg)\n",
    "\n",
    "#transfrom to pandas\n",
    "df_cat = indexed.toPandas()\n",
    "\n",
    "#TRAINING-VALIDATION (0.4-0.6)\n",
    "#Split the DOIs for training and Test-----random state =42\n",
    "\n",
    "X_train, X_validation, y_train, y_validation  = train_test_split(df_cat['DOI'],df_cat['label'], test_size=0.6, random_state=42)\n",
    "\n",
    "\n",
    "#KEEP IT FOR VECTORIZE LATER\n",
    "second_split_back_up = X_validation\n",
    "\n",
    "\n",
    "\n",
    "#VALIDATION-TEST (0.66-0.34)--->(0.4,0.2) from original dataset\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_validation, y_validation, test_size=0.34, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_validation))\n",
    "print(len(X_test))\n",
    "print(len(second_split_back_up))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"Section\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "count_vec = CountVectorizer(inputCol='filtered', outputCol='count_vec')\n",
    "idf = IDF(inputCol='count_vec', outputCol='features',minDocFreq=2)\n",
    "\n",
    "#STRINGINDEXER THE DOC_ID\n",
    "\n",
    "int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "int_sections = StringIndexer(inputCol = 'doc_id', outputCol ='uni_sec_id')\n",
    "\n",
    "\n",
    "prep_pipeline = Pipeline(stages = [int_category, int_sections, regexTokenizer, remover, count_vec, idf])\n",
    "pre_processing = prep_pipeline.fit(df)\n",
    "pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "final_data = pre_processed_data.select(['doc_id','uni_sec_id','DOI','label','features']).cache()\n",
    "# display(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol='Section', outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "count_vec = CountVectorizer(inputCol='filtered', outputCol='features')\n",
    "\n",
    "int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "int_id = StringIndexer(inputCol='doc_id',outputCol='id') #map each unique document with an id\n",
    "#PIPELINE\n",
    "\n",
    "\n",
    "prep_pipeline = Pipeline(stages = [int_category, int_id, regexTokenizer, remover, count_vec])\n",
    "\n",
    "pre_processing = prep_pipeline.fit(df)\n",
    "pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "final_data_LDA = pre_processed_data.cache()\n",
    "\n",
    "\n",
    "\n",
    "final_data_LDA = final_data_LDA.dropDuplicates(['id'])\n",
    "# final_data_LDA = final_data_LDA.select(['id','DOI','contENT','features','Section','Category','Sections','label'])\n",
    "# final_data_LDA.count()\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=50, maxIter=50, optimizer=\"em\")\n",
    "model = lda.fit(final_data_LDA.select('id','features'))\n",
    "\n",
    "\n",
    "#Create LDA features\n",
    "transformed = model.transform(final_data_LDA)\n",
    "final_data = transformed.select(['DOI','label','topicDistribution']).withColumnRenamed(\"topicDistribution\", \"features\").cache()\n",
    "display(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier,\n",
    "                                      DecisionTreeClassifier)\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Multinomial Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "\n",
    "#Decision Tree - Random Forest \n",
    "dtc = DecisionTreeClassifier(maxDepth = 10)\n",
    "# maxDepth = 15\n",
    "rfc = RandomForestClassifier(numTrees = 150, maxDepth = 10)\n",
    "\n",
    "#Linear SVM - One vs All\n",
    "lsvc = LinearSVC(maxIter=15, regParam=0.15)\n",
    "ovr = OneVsRest(classifier=lsvc  )\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.1, elasticNetParam=0.1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Split Training - Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "train = final_data.filter(final_data.DOI.isin(list(X_train)))\n",
    "\n",
    "#validation\n",
    "validation = final_data.filter(final_data.DOI.isin(list(X_validation)))\n",
    "\n",
    "\n",
    "#For strong classifier Ensemble\n",
    "sec_classifir_vec  = final_data.filter(final_data.DOI.isin(list(second_split_back_up)))\n",
    "\n",
    "print(train.count())\n",
    "print(validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_measures(new_df_p):\n",
    "    '''\n",
    "    Predict all the evaluation Measures:\n",
    "    Accuracy, F1-Score, Recall, Precision\n",
    "    '''\n",
    "    \n",
    "    ev_m = []\n",
    "    \n",
    "    #ACCURACY\n",
    "    acc_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName = 'accuracy')\n",
    "    accuracy = acc_eval.evaluate(new_df_p)\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    ev_m.append(accuracy)\n",
    "    \n",
    "    #F1-SCORE\n",
    "    acc_eval_f1 = MulticlassClassificationEvaluator(metricName = 'f1')\n",
    "    f1_score = acc_eval_f1.evaluate(new_df_p)\n",
    "    print(\"f1 = %g\" % f1_score)\n",
    "    ev_m.append(f1_score)\n",
    "    \n",
    "    #RECALL\n",
    "    acc_eval_recall = MulticlassClassificationEvaluator(metricName = 'weightedRecall')\n",
    "    recall = acc_eval_recall.evaluate(new_df_p)\n",
    "    print(\"weightedRecall = %g\" % recall)\n",
    "    ev_m.append(recall)\n",
    "    \n",
    "    #PRECISION\n",
    "    acc_eval_precission = MulticlassClassificationEvaluator(metricName = \"weightedPrecision\")\n",
    "    precission = acc_eval_precission.evaluate(new_df_p)\n",
    "    print(\"weightedPrecision = %g\" % precission)\n",
    "    ev_m.append(precission)\n",
    "    \n",
    "    return ev_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train classifier - train set\n",
    "paper_class = dtc.fit(train)\n",
    "\n",
    "#Prepare features for the second Classifier\n",
    "df_Second = paper_class.transform(sec_classifir_vec)\n",
    "\n",
    "#predict labels - validation set\n",
    "validation_results = paper_class.transform(validation)\n",
    "evaluation_measures(validation_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check sections order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximum(test_results):\n",
    "    '''\n",
    "    Finds the maximum probability and \n",
    "    prepares the dataframe for the Majority Vote.\n",
    "    '''  \n",
    "    udf_wf_var = udf(lambda x: round(np.max(np.array(x)),10), returnType=FloatType()) #Define UDF function\n",
    "    df =  test_results.withColumn('WF_Var',udf_wf_var('probability')).dropna()\n",
    "\n",
    "    return df.groupby(\"DOI\").agg(F.collect_list('prediction').alias('list_categ'),F.collect_list('WF_Var').alias('prob_categ'),F.collect_list('doc_id').alias('Sec_ids'))\n",
    "\n",
    "test_df_sect = find_maximum(validation_results)\n",
    "display(test_df_sect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Vectorized Dataframe for the Second Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import (LongType ,StringType, IntegerType,\n",
    "                               FloatType, DoubleType, ArrayType)\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "\n",
    "def find_maximum(test_results):\n",
    "    '''\n",
    "    Finds the maximum probability and \n",
    "    prepares the dataframe for the Majority Vote.\n",
    "    '''  \n",
    "    udf_wf_var = udf(lambda x: round(np.max(np.array(x)),10), returnType=FloatType()) #Define UDF function\n",
    "    df =  test_results.withColumn('WF_Var',udf_wf_var('probability')).dropna()\n",
    "\n",
    "    return df.groupby(\"DOI\").agg(F.collect_list('prediction').alias('list_categ'),F.collect_list('WF_Var').alias('prob_categ'))\n",
    "\n",
    "\n",
    "def filter_arrays(ar):\n",
    "    '''\n",
    "    Returns only the arrays that match with \n",
    "    the array_length.\n",
    "    '''\n",
    "    array_length = 4\n",
    "    \n",
    "    if len(ar)!=array_length:\n",
    "          return int(0)\n",
    "    else:\n",
    "          return int(1)\n",
    "    \n",
    "\n",
    "def clear_corrupt_vectors(new_set):\n",
    "    '''\n",
    "    Filter out corrupt or unmatching \n",
    "    vector lengths by using the filter_arrays\n",
    "    function. The function has a fixed length of 4\n",
    "    need to be changed if needs to be tested higher vectors.\n",
    "    '''\n",
    "    flt = udf( filter_arrays,IntegerType())\n",
    "    filt_df =new_set.withColumn('binary_filter',flt(new_set.prob_categ))\n",
    "\n",
    "    return filt_df.filter(col('binary_filter').isin([1]))\n",
    "\n",
    "\n",
    "def vectorize_array(df):\n",
    "    '''\n",
    "    Vectorize a dataframe's column with arrays.\n",
    "    Preparing the dataframe for the classifier.\n",
    "    '''\n",
    "    to_vector = udf(lambda a: Vectors.dense(a), VectorUDT())\n",
    "    \n",
    "    return df.select('DOI', to_vector(\"prob_categ\").alias(\"features\"))\n",
    "    \n",
    "\n",
    "def prepare_for_strong_classifier(test_results):\n",
    "    '''\n",
    "    Uses all the above functions to transform\n",
    "    '''\n",
    "    \n",
    "    #Find Maximum Probability and prepare dataframe for majority vote\n",
    "    df_prep = find_maximum(test_results)\n",
    "\n",
    "    #prepare the dataset to send to the stronger classifier\n",
    "    return vectorize_array(clear_corrupt_vectors(df_prep))\n",
    "    \n",
    "\n",
    "df_train_2 = prepare_for_strong_classifier(df_Second)#TOD-DO CHECK IF THE COLUMSN ARE RIGHT ADN IS THE CORRECT DATAFRAME!!\n",
    "\n",
    "# Inner Merge with real Categories\n",
    "df_train_2 = df_train_2.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna().cache()\n",
    "display(df_train_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training_Set_2 - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT DO IT FOR THE SECOND EXPERIMENT\n",
    "\n",
    "#validation\n",
    "validation_2 = df_train_2.filter(df_train_2.DOI.isin(list(X_validation)))\n",
    "\n",
    "#test\n",
    "test_df_f = df_train_2.filter(df_train_2.DOI.isin(list(X_test)))\n",
    "\n",
    "print(validation_2.count())\n",
    "print(test_df_f.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all Second Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_algo={'nb':'Multinomial Naive Bayes',\n",
    "           'lr':'Multinomial Logistic Regression',\n",
    "           'dtc':'Decision Tree Classifier',\n",
    "           'rfc':'Random Forest Classifier',\n",
    "           'ovr':'One vs Rest Linear SVM'\n",
    "}\n",
    "\n",
    "\n",
    "algorithms = [dtc, nb,lr,ovr,rfc]\n",
    "names = [\"dtc\", \"nb\", 'lr', \"ovr\",'rfc']\n",
    "for algo, n in zip(algorithms,names):\n",
    "    print(dict_algo[n])\n",
    "    paper_class_2 = algo.fit(validation_2)\n",
    "    df_Second_results = paper_class_2.transform(test_df_f)\n",
    "    evaluation_measures(df_Second_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train classifier - train set\n",
    "paper_class_2 = nb.fit(validation_2)\n",
    "\n",
    "#predict labels - test set\n",
    "df_Second_results = paper_class_2.transform(test)\n",
    "# display(df_Second_results)\n",
    "#EVALUATE\n",
    "evaluation_measures(df_Second_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple majority vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import random \n",
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def prepare_rand_majority(df):\n",
    "  '''\n",
    "  Return a dataframe which contains\n",
    "  in each row a document and a list of\n",
    "  the predictions of the classifier prepared \n",
    "  for the majority vote.\n",
    "  '''\n",
    "  return  df.groupby(\"DOI\").agg(F.collect_list(\"prediction\").alias('list_categ'))\n",
    "\n",
    "\n",
    "def majority_vote_random(votes):\n",
    "    '''\n",
    "    Predict the majority vote by\n",
    "    returning randomly when we have more\n",
    "    than one winner class.\n",
    "    '''\n",
    "\n",
    "    vote_counts = {}\n",
    "    \n",
    "    for vote in votes:\n",
    "        if vote in vote_counts:\n",
    "            vote_counts[vote] += 1\n",
    "        else:\n",
    "            vote_counts[vote] = 1\n",
    "    winners = []\n",
    "    max_count = np.max(vote_counts.values())\n",
    "    \n",
    "    for vote, count in vote_counts.items():\n",
    "        if count == max_count:\n",
    "            winners.append(vote)\n",
    "            \n",
    "    return random.choice(winners)\n",
    "\n",
    "  \n",
    "def final_prediction_maj_random(df):\n",
    "    '''\n",
    "    Computes the final predictions based\n",
    "    on a simple majority vote.\n",
    "    '''\n",
    "    df_for_majority_vote = prepare_rand_majority(df)\n",
    "    #udf apply function\n",
    "    majority_function = udf(lambda z: majority_vote_random(z), DoubleType())\n",
    "#     final_prediction = df_for_majority_vote.withColumn('prediction', majority_function(col('list_categ')))\n",
    "    f_res = df_for_majority_vote.withColumn('prediction', majority_function(col('list_categ')))\n",
    "    return f_res.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna().cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "Ensemble_Notebook",
  "notebookId": 1001257
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
