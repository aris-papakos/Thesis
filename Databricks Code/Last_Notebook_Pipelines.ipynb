{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (RegexTokenizer, Tokenizer, HashingTF, IDF,\n",
    "                                StopWordsRemover, CountVectorizer, StopWordsRemover, StringIndexer, OneHotEncoder)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                  MulticlassClassificationEvaluator)\n",
    "from pyspark.sql.types import (LongType ,StringType, IntegerType,\n",
    "                               FloatType, DoubleType, ArrayType)\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook is consists of pyspark code. It is necessary first to be uploaded on Databricks as well all the appropriate tables to have been created. For creating all the appropriate tables for this notebook from the stored csv in the S3 bucket use the **Create_appropriate_tables.ipynb** notebook in Databricks. The path of the csvs in the Elseviers S3 bucket is  **\"---------------------------\" #Path has removed because of confidentiality reasons**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DOIs and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the Mapping Dataframe for the 4 Categories \n",
    "df_doiCateg = spark.sql(\"SELECT * FROM taxiarchis.doi_categories_4\")\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"label\")\n",
    "\n",
    "#FOR ENSEMBLE EXPERIMENT ONLY!!!\n",
    "# df_doiCateg = spark.sql(\"SELECT * FROM taxiarchis.doi_categories_4_4_features\")\n",
    "# indexer = StringIndexer(inputCol=\"Category\", outputCol=\"label\")\n",
    "\n",
    "\n",
    "indexed = indexer.fit(df_doiCateg).transform(df_doiCateg)\n",
    "\n",
    "#transfrom to pandas\n",
    "df_cat = indexed.toPandas()\n",
    "\n",
    "#Split the DOIs for training and Test ----- randomstate =42\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cat['DOI'],df_cat['label'] , test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL DOCUMENT ----- UNCOMMENT AND COMMENT OUT IF YOU WANT TO USE THE REST\n",
    "\n",
    "# df = spark.sql(\"SELECT * FROM taxiarchis.fulldocument_4\")\n",
    "# df = df.dropna()\n",
    "# df.count()\n",
    "\n",
    "\n",
    "#PER SECTION DOCUMENTS ----- UNCOMMENT AND COMMENT OUT IF YOU WANT TO USE THE REST\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM taxiarchis.persection_4_1\")\n",
    "df = df.dropna()\n",
    "df.count()\n",
    "\n",
    "#FOR ENSEMBLE ----- UNCOMMENT AND COMMENT OUT IF YOU WANT TO USE THE REST\n",
    "\n",
    "# df = spark.sql(\"SELECT * FROM taxiarchis.section_4_4_ensemble\").dropna()\n",
    "# df = df.dropna()\n",
    "# df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency - inverse term frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fullDocument(df):\n",
    "    '''\n",
    "    Vectorize full documents as a\n",
    "    tf-idf numerical text representation\n",
    "    '''\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Full_Document\", outputCol=\"tokens\", pattern=\"\\\\W\") #Tokenize\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered') #Remove stop words\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='count_vec') #Create Bag-of-word (TF)\n",
    "    idf = IDF(inputCol='count_vec', outputCol='features')#Weight vetor (idf)\n",
    "\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Categories',outputCol='label')\n",
    "    int_sections = StringIndexer(inputCol = 'DOI', outputCol ='uni_id')\n",
    "\n",
    "    #Create a pipeline\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_sections, regexTokenizer, remover, count_vec, idf])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    return pre_processed_data.select(['Categories','uni_id','DOI','label','features'])\n",
    "\n",
    "\n",
    "# final_data = tfidf_fullDocument(df)\n",
    "# final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tfidf_section(df):\n",
    "      \n",
    "    '''\n",
    "    Vectorize sections in tf-idf text\n",
    "    Representation.\n",
    "    '''\n",
    "\n",
    "    #FOR THE FULL DOCUMENT NEED TO BE CHANGED!!\n",
    "\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Section\", outputCol=\"tokens\", pattern=\"\\\\W\")#Tokenize\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered') #Remove stop words\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='count_vec') #Create Bag-of-word (TF)\n",
    "    idf = IDF(inputCol='count_vec', outputCol='features',minDocFreq=2) #Weight vetor (idf)\n",
    "\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "    int_sections = StringIndexer(inputCol = 'doc_id', outputCol ='uni_sec_id') \n",
    "\n",
    "    #Create a pipeline\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_sections, regexTokenizer, remover, count_vec, idf])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    return pre_processed_data.select(['doc_id','uni_sec_id','DOI','label','features'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_fullDocument(df,numTopics = 30):\n",
    "  \n",
    "    '''\n",
    "    Vectorize full document's text by representing them \n",
    "    with lda features. The function returns a dataframe.\n",
    "    '''\n",
    "    int_category = StringIndexer(inputCol='Categories',outputCol='label')\n",
    "    int_id = StringIndexer(inputCol='DOI',outputCol='id') #map each unique document with an id\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Full_Document\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='features')\n",
    "\n",
    "    #PIPELINE\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_id, regexTokenizer, remover, count_vec])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    final_data_LDA = pre_processing.transform(df)\n",
    "\n",
    "    final_data_LDA = final_data_LDA.dropDuplicates(['id'])\n",
    "    final_data_LDA = final_data_LDA.select(['DOI','id','features','Full_Document','Categories','label'])\n",
    "\n",
    "\n",
    "    # Trains a LDA model.\n",
    "    lda = LDA(k=numTopics, maxIter=50, optimizer=\"em\")\n",
    "    model = lda.fit(final_data_LDA.select('id','features'))\n",
    "\n",
    "\n",
    "    #Create LDA features\n",
    "    transformed = model.transform(final_data_LDA)\n",
    "    \n",
    "    return transformed.select(['DOI','label','topicDistribution']).withColumnRenamed(\"topicDistribution\", \"features\")\n",
    "\n",
    "# final_data = LDA_fullDocument(df)\n",
    "# final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_representation_sections(df,numTopics = 2):\n",
    "  \n",
    "    '''\n",
    "    Vectorize documents of sections by representing them \n",
    "    in lda text representation. The function returns a dataframe.\n",
    "    '''\n",
    "  \n",
    "    #FOR THE FULL DOCUMENT NEED TO BE CHANGED!!\n",
    "    regexTokenizer = RegexTokenizer(inputCol='Section', outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='features')\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "    int_id = StringIndexer(inputCol='doc_id',outputCol='id') #map each unique scetion with an unique id -Very important in order the lda to work\n",
    "  \n",
    "\n",
    "    #PIPELINE\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_id, regexTokenizer, remover, count_vec])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    final_data_LDA = pre_processed_data\n",
    "    final_data_LDA = final_data_LDA.dropDuplicates(['id'])\n",
    "  \n",
    " \n",
    "    # Train a LDA model.\n",
    "    lda = LDA(k=numTopics, maxIter=50, optimizer=\"em\")\n",
    "    model = lda.fit(final_data_LDA.select('id','features'))\n",
    "\n",
    "\n",
    "    #Create LDA features\n",
    "    transformed = model.transform(final_data_LDA)\n",
    "    \n",
    "    return transformed.select(['DOI','label','topicDistribution']).withColumnRenamed(\"topicDistribution\", \"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_measures(new_df_p):\n",
    "    '''\n",
    "    Predict all the evaluation Measures:\n",
    "    Accuracy, F1-Score, Recall, Precision\n",
    "    '''\n",
    "    \n",
    "    ev_m = []\n",
    "    \n",
    "    #ACCURACY\n",
    "    acc_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName = 'accuracy')\n",
    "    accuracy = acc_eval.evaluate(new_df_p)\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    \n",
    "    return accuracy\n",
    "#     ev_m.append(accuracy)\n",
    "    \n",
    "#     #F1-SCORE\n",
    "#     acc_eval_f1 = MulticlassClassificationEvaluator(metricName = 'f1')\n",
    "#     f1_score = acc_eval_f1.evaluate(new_df_p)\n",
    "#     print(\"f1 = %g\" % f1_score)\n",
    "#     ev_m.append(f1_score)\n",
    "    \n",
    "#     #RECALL\n",
    "#     acc_eval_recall = MulticlassClassificationEvaluator(metricName = 'weightedRecall')\n",
    "#     recall = acc_eval_recall.evaluate(new_df_p)\n",
    "#     print(\"weightedRecall = %g\" % recall)\n",
    "#     ev_m.append(recall)\n",
    "    \n",
    "#     #PRECISION\n",
    "#     acc_eval_precission = MulticlassClassificationEvaluator(metricName = \"weightedPrecision\")\n",
    "#     precission = acc_eval_precission.evaluate(new_df_p)\n",
    "#     print(\"weightedPrecision = %g\" % precission)\n",
    "#     ev_m.append(precission)\n",
    "    \n",
    "#     return ev_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def micro(true, predict):\n",
    "  \n",
    "  t = precision_recall_fscore_support(true, predict, average='micro')\n",
    "  \n",
    "  return (round(t[0],4)*100, round(t[1],4)*100, round(t[2],4)*100)\n",
    "  \n",
    "def macro(true, predict):\n",
    "  \n",
    "  t2 = precision_recall_fscore_support(true, predict, average='macro')\n",
    "  \n",
    "  return (round(t2[0],4)*100, round(t2[1],4)*100, round(t2[2],4)*100)\n",
    "\n",
    "def weighted(true, predict):\n",
    "  \n",
    "  t3 = precision_recall_fscore_support(true, predict, average='weighted')\n",
    "  \n",
    "  return (round(t3[0],4)*100, round(t3[1],4)*100, round(t3[2],4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms  - with their hyper-parameters tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier,\n",
    "                                      DecisionTreeClassifier)\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Multinomial Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "\n",
    "#Decision Tree - Random Forest \n",
    "dtc = DecisionTreeClassifier(maxDepth = 12)\n",
    "\n",
    "#Random Forest\n",
    "rfc = RandomForestClassifier(numTrees = 200, maxDepth = 15)\n",
    "\n",
    "#Linear SVM - One vs All\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.2)\n",
    "ovr = OneVsRest(classifier=lsvc  )\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(maxIter=15, regParam=0.0, elasticNetParam=0.0 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training - Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(final_data):\n",
    "    '''\n",
    "    Splits data in training(70%) and test set(30%)\n",
    "    '''\n",
    "    #train\n",
    "    train = final_data.filter(final_data.DOI.isin(list(X_train)))\n",
    "    #test\n",
    "    test = final_data.filter(final_data.DOI.isin(list(X_test)))\n",
    "    \n",
    "    return (train, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from pyspark.sql.functions import udf, max\n",
    "from pyspark.sql.types import LongType,StringType,IntegerType, FloatType,DoubleType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "def find_maximum(test_results):\n",
    "  \n",
    "    '''\n",
    "    Finds the maximum probability and \n",
    "    prepares the dataframe for the Majority Vote.\n",
    "    '''\n",
    "    udf_wf_var = udf(lambda x: round(np.max(np.array(x)),10), returnType=FloatType()) #Define UDF function\n",
    "    df =  test_results.withColumn('WF_Var',udf_wf_var('probability')).dropna()\n",
    "\n",
    "    return df.groupby(\"DOI\").agg(F.collect_list('prediction').alias('list_categ'),F.collect_list('WF_Var').alias('prob_categ'))\n",
    "\n",
    "\n",
    "def majority_vote_percent(pre,prob):\n",
    "    '''\n",
    "    Predict the majority vote based on the given \n",
    "    probabilities  of the classifier by\n",
    "    returning randomly a class when we have more\n",
    "    than one winner.\n",
    "    '''\n",
    "    \n",
    "    maj_vote ={}\n",
    "\n",
    "    for prdi, prba in zip(pre,prob):\n",
    "        \n",
    "        \n",
    "        if prdi in maj_vote:\n",
    "            maj_vote[prdi] += prba    \n",
    "        else:\n",
    "            maj_vote[prdi] = prba\n",
    "        \n",
    "    winners = []\n",
    "    max_count = np.max(maj_vote.values())  \n",
    "    for vote, count in maj_vote.items():\n",
    "        if count == max_count:\n",
    "            winners.append(vote)\n",
    "    \n",
    "    return random.choice(winners)\n",
    "  \n",
    "def max_probability_class(pre,prob):\n",
    "      \n",
    "    '''\n",
    "    Returns the class with the maximum \n",
    "    section probability \n",
    "    '''\n",
    "    max_pro = 0.0\n",
    "    max_class = 0\n",
    "    for prdi, prba in zip(pre,prob):\n",
    "        if prba > max_pro:\n",
    "            max_pro = prba\n",
    "            max_class = prdi\n",
    "\n",
    "    return max_class\n",
    "  \n",
    "    \n",
    "def majority_final_prediction(test_results):\n",
    "\n",
    "    '''\n",
    "    Makes the final prediction based on the majority vote\n",
    "    results of probabilities.\n",
    "    '''\n",
    "    #Find Maximum Probability and prepare dataframe for majority vote\n",
    "    df_for_majority_vote = find_maximum(test_results)\n",
    "\n",
    "    #create UDF function\n",
    "    majority_function_probab = udf(majority_vote_percent, DoubleType())\n",
    "\n",
    "    final_prediction=df_for_majority_vote.withColumn('prediction',majority_function_probab(df_for_majority_vote.list_categ, df_for_majority_vote.prob_categ))\n",
    "\n",
    "    # Inner Merge with real Categories\n",
    "    return final_prediction.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import random \n",
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def prepare_rand_majority(df):\n",
    "  \n",
    "    '''\n",
    "    Return a dataframe which contains\n",
    "    in each row a document and a list of\n",
    "    the predictions of the classifier prepared \n",
    "    for the majority vote.\n",
    "    '''\n",
    "    return  df.groupby(\"DOI\").agg(F.collect_list(\"prediction\").alias('list_categ'))\n",
    "\n",
    "\n",
    "def majority_vote_random(votes):\n",
    "    '''\n",
    "    Predict the majority vote by\n",
    "    returning randomly when we have more\n",
    "    than one winner class.\n",
    "    '''\n",
    "\n",
    "    vote_counts = {}\n",
    "    \n",
    "    for vote in votes:\n",
    "        if vote in vote_counts:\n",
    "            vote_counts[vote] += 1\n",
    "        else:\n",
    "            vote_counts[vote] = 1\n",
    "    winners = []\n",
    "    max_count = np.max(vote_counts.values())\n",
    "    \n",
    "    for vote, count in vote_counts.items():\n",
    "        if count == max_count:\n",
    "            winners.append(vote)\n",
    "            \n",
    "    return random.choice(winners)\n",
    "\n",
    "  \n",
    "def final_prediction_maj_random(df):\n",
    "    '''\n",
    "    Computes the final predictions based\n",
    "    on a simple majority vote.\n",
    "    '''\n",
    "    df_for_majority_vote = prepare_rand_majority(df)\n",
    "    #udf apply function\n",
    "    majority_function = udf(lambda z: majority_vote_random(z), DoubleType())\n",
    "    f_res = df_for_majority_vote.withColumn('prediction', majority_function(col('list_categ')))\n",
    "    return f_res.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together- make predictions - Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF REPRESENTATION\n",
    "final_data =  tfidf_fullDocument(df)\n",
    "train, test = split_train_test(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA text REPRESENTATION----- UNCOMMENT AND COMMENT OUT TF-IDF IF YOU WANT TO USE LDA\n",
    "\n",
    "# print('Train LDA Algorithm...')\n",
    "# final_data = LDA_fullDocument(df, 50)\n",
    "# print('LDA Algorithm has been trained.')\n",
    "# train, test = split_train_test(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_algo={'nb':'Multinomial Naive Bayes',\n",
    "           'lr':'Multinomial Logistic Regression',\n",
    "           'dtc':'Decision Tree Classifier',\n",
    "           'rfc':'Random Forest Classifier',\n",
    "           'ovr':'One vs Rest Linear SVM'}\n",
    "\n",
    "algorithms = [nb,lr,ovr,dtc,rfc]\n",
    "names = [ \"nb\", 'lr','ovr',\"dtc\",'rfc']\n",
    "measures = {'Measures':['Accuracy','Macro-Precision','Macro-Recall','Macro-FScore','Micro-Precision','Micro-Recall','Micro-FScore','Weighted-Precision','Weighted-Recall','Weighted-FScore']}\n",
    "\n",
    "\n",
    "conf_matices = {}\n",
    "predict_values = {}\n",
    "\n",
    "\n",
    "for algo, n in zip(algorithms,names):\n",
    "  \n",
    "    print(dict_algo[n])\n",
    "    #train\n",
    "    paper_class_2 = algo.fit(train)\n",
    "    test_results = paper_class_2.transform(test)\n",
    "\n",
    "    #Write a table in databricks for each algorithm\n",
    "    s = 'taxiarchis.'+n+'_lda_balancedfull'\n",
    "    test_results.write.mode(\"overwrite\").saveAsTable(s)\n",
    "\n",
    "    colle = test_results.select(['label', 'prediction']).collect()\n",
    "\n",
    "    tru_value = [int(i.label) for i in colle]#Collect the true label in a list \n",
    "    predict_value = [int(i.prediction) for i in colle] #Collect the  prediction in a list \n",
    "\n",
    "    #Compute evaluation scores\n",
    "    meas = []\n",
    "    meas.append(round(evaluation_measures(test_results),4)*100)\n",
    "    meas.extend(macro(tru_value,predict_value))\n",
    "    meas.extend(micro(tru_value,predict_value))\n",
    "    meas.extend(weighted(tru_value,predict_value))\n",
    "\n",
    "    measures[n] = meas\n",
    "    print(meas)\n",
    "\n",
    "    y_actu = pd.Series(tru_value, name='Actual')\n",
    "    y_pred = pd.Series(predict_value, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred)  #Confusion Matrix\n",
    "\n",
    "    conf_matices[n] = df_confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df_fullDocument = pd.DataFrame(conf_matices['nb'])\n",
    "pd_df_fullDocument=pd_df_fullDocument.set_index('Measures')\n",
    "pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']]\n",
    "\n",
    "\n",
    "pd_df_fullDocument\n",
    "\n",
    "# print (pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all the confusion matrices for each algorithm in a latex format\n",
    "for i  in  dict_algo.keys():\n",
    "    print(str(dict_algo[i]))\n",
    "    print(conf_matices[i].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Based - Simple Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF REPRESENTATION\n",
    "final_data =  prepare_tfidf_section(df)\n",
    "train, test = split_train_test(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA REPRESENTATION ----- UNCOMMENT AND COMMENT OUT TF-IDF IF YOU WANT TO USE LDA\n",
    "\n",
    "# print('Train LDA Algorithm...')\n",
    "# final_data = lda_representation_sections(df, 50)\n",
    "# print('LDA Algorithm has been trained.')\n",
    "\n",
    "# train, test = split_train_test(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_algo={'nb':'Multinomial Naive Bayes',\n",
    "           'lr':'Multinomial Logistic Regression',\n",
    "           'dtc':'Decision Tree Classifier',\n",
    "           'rfc':'Random Forest Classifier',\n",
    "           'ovr':'One vs Rest Linear SVM'}\n",
    "\n",
    "algorithms = [nb, lr, ovr, dtc,rfc]\n",
    "names = [ \"nb\", 'lr','ovr', \"dtc\",'rfc']\n",
    "\n",
    "measures = {'Measures':['Accuracy','Macro-Precision','Macro-Recall','Macro-FScore','Micro-Precision','Micro-Recall','Micro-FScore','Weighted-Precision','Weighted-Recall','Weighted-FScore']}\n",
    "\n",
    "conf_matices = {}\n",
    "predict_values = {}\n",
    "\n",
    "\n",
    "for algo, n in zip(algorithms,names):\n",
    "  \n",
    "    print(dict_algo[n])\n",
    "    #train\n",
    "    paper_class_2 = algo.fit(train)\n",
    "    test_results = paper_class_2.transform(test)\n",
    "    new_df = final_prediction_maj_random(test_results)#Simple Majority Vote\n",
    "\n",
    "    s = 'taxiarchis.'+n+'_tfidf_simpleMajority'\n",
    "    #   s = 'taxiarchis.'+n+'_lda_simpleMajority'\n",
    "    new_df.write.mode(\"overwrite\").saveAsTable(s) \n",
    "\n",
    "\n",
    "    colle = new_df.select(['label', 'prediction']).collect()\n",
    "\n",
    "    tru_value = [int(i.label) for i in colle]#Collect the true label in a list \n",
    "    predict_value = [int(i.prediction) for i in colle] #Collect the  prediction in a list \n",
    "\n",
    "    #Compute evaluation measures\n",
    "    meas = []\n",
    "    meas.append(round(evaluation_measures(new_df),4)*100)\n",
    "    meas.extend(macro(tru_value,predict_value))\n",
    "    meas.extend(micro(tru_value,predict_value))\n",
    "    meas.extend(weighted(tru_value,predict_value))\n",
    "\n",
    "    measures[n] = meas\n",
    "    print(meas)\n",
    "\n",
    "    y_actu = pd.Series(tru_value, name='Actual')\n",
    "    y_pred = pd.Series(predict_value, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred)  #Confusion Matrix\n",
    "    print(df_confusion)\n",
    "    conf_matices[n] = df_confusion\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df_fullDocument = pd.DataFrame(conf_matices['nb'])\n",
    "pd_df_fullDocument=pd_df_fullDocument.set_index('Measures')\n",
    "pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']]\n",
    "\n",
    "\n",
    "pd_df_fullDocument\n",
    "\n",
    "# print (pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all the confusion matrices for each algorithm in a latex format\n",
    "for i  in  dict_algo.keys():\n",
    "  print(str(dict_algo[i]))\n",
    "  print(conf_matices[i].to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Based - Probabilistic Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF REPRESENTATION\n",
    "final_data =  prepare_tfidf_section(df)\n",
    "train, test = split_train_test(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA REPRESENTATION ----- UNCOMMENT AND COMMENT OUT TF-IDF IF YOU WANT TO USE LDA\n",
    "\n",
    "# print('Train LDA Algorithm...')\n",
    "# final_data = lda_representation_sections(df, 50)\n",
    "# print('LDA Algorithm has been trained.')\n",
    "\n",
    "# train, test = split_train_test(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_algo={'nb':'Multinomial Naive Bayes',\n",
    "           'lr':'Multinomial Logistic Regression',\n",
    "           'dtc':'Decision Tree Classifier',\n",
    "           'rfc':'Random Forest Classifier',\n",
    "           'ovr':'One vs Rest Linear SVM'}\n",
    "\n",
    "algorithms = [nb, lr, ovr, dtc,rfc]\n",
    "names = [ \"nb\", 'lr','ovr', \"dtc\",'rfc']\n",
    "\n",
    "measures = {'Measures':['Accuracy','Macro-Precision','Macro-Recall','Macro-FScore','Micro-Precision','Micro-Recall','Micro-FScore','Weighted-Precision','Weighted-Recall','Weighted-FScore']}\n",
    "\n",
    "conf_matices = {}\n",
    "predict_values = {}\n",
    "\n",
    "\n",
    "for algo, n in zip(algorithms,names):\n",
    "  \n",
    "    print(dict_algo[n])\n",
    "    #train\n",
    "    paper_class_2 = algo.fit(train)\n",
    "    test_results = paper_class_2.transform(test)\n",
    "    new_df = majority_final_prediction(test_results) #Majority Vote Based on the probabilities given by the classifier\n",
    "\n",
    "    s = 'taxiarchis.'+n+'_tfidf_probabilisticMajority'\n",
    "    #   s = 'taxiarchis.'+n+'_lda_probabilisticMajority'\n",
    "    new_df.write.mode(\"overwrite\").saveAsTable(s) \n",
    "\n",
    "\n",
    "    colle = new_df.select(['label', 'prediction']).collect()\n",
    "\n",
    "    tru_value = [int(i.label) for i in colle]#Collect the true label in a list \n",
    "    predict_value = [int(i.prediction) for i in colle] #Collect the  prediction in a list \n",
    "\n",
    "    #Compute evaluation measures\n",
    "    meas = []\n",
    "    meas.append(round(evaluation_measures(new_df),4)*100)\n",
    "    meas.extend(macro(tru_value,predict_value))\n",
    "    meas.extend(micro(tru_value,predict_value))\n",
    "    meas.extend(weighted(tru_value,predict_value))\n",
    "\n",
    "    measures[n] = meas\n",
    "    print(meas)\n",
    "\n",
    "    y_actu = pd.Series(tru_value, name='Actual')\n",
    "    y_pred = pd.Series(predict_value, name='Predicted')\n",
    "    df_confusion = pd.crosstab(y_actu, y_pred)  #Confusion Matrix\n",
    "    print(df_confusion)\n",
    "    conf_matices[n] = df_confusion\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df_fullDocument = pd.DataFrame(conf_matices['nb'])\n",
    "pd_df_fullDocument=pd_df_fullDocument.set_index('Measures')\n",
    "pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']]\n",
    "\n",
    "\n",
    "pd_df_fullDocument\n",
    "\n",
    "# print (pd_df_fullDocument[['nb','lr','ovr','dtc','rfc']].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all the confusion matrices for each algorithm in a latex format\n",
    "for i  in  dict_algo.keys():\n",
    "    print(str(dict_algo[i]))\n",
    "    print(conf_matices[i].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "Last_Notebook_Final",
  "notebookId": 1016477
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
