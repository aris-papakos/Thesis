{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (RegexTokenizer, Tokenizer, HashingTF, IDF,\n",
    "                                StopWordsRemover, CountVectorizer, StopWordsRemover, StringIndexer, OneHotEncoder)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                  MulticlassClassificationEvaluator)\n",
    "from pyspark.sql.types import (LongType ,StringType, IntegerType,\n",
    "                               FloatType, DoubleType, ArrayType)\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full text documents\n",
    "df = spark.sql(\"SELECT * FROM taxiarchis.fulldocument_4\")\n",
    "df = df.dropna()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per Section Documents\n",
    "df1 = spark.sql(\"SELECT * FROM taxiarchis.persection_4_1\")\n",
    "df1 = df1.dropna()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "per_cla = {'Classes':['all','0','1','2','3']\n",
    "  \n",
    "}\n",
    "\n",
    "cate = [0.0, 1.0, 2.0, 3.0]\n",
    "names = [ \"nb\", 'lr','ovr',\"dtc\",'rfc']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def take_f1_scores(true, predict):\n",
    "    '''\n",
    "    Return the weighted-average f1-score\n",
    "    '''\n",
    "    return f1_score(true, predict, average=\"weighted\")  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def oracle(label,prediction, list_predictions):\n",
    "    '''\n",
    "    If there are a least one correct prediction among the sections,\n",
    "    replace it with as True Positive fdocument's class\n",
    "    '''\n",
    "    if label in list_predictions:\n",
    "        return int(label)\n",
    "    else:\n",
    "        return int(prediction)\n",
    "\n",
    "def oracle predictions(method_combination):\n",
    "    '''\n",
    "    Explore the posibilities of the section-based methodology\n",
    "    by spesifying the upperbounds of this approach.\n",
    "    '''\n",
    "\n",
    "    for algo in names:\n",
    "    print(str(algo))\n",
    "\n",
    "    #Finds the saved tables with th predictions of each algorithm\n",
    "    #SQL query for retreiving the saved tables in Databricks\n",
    "    s = 'SELECT * FROM taxiarchis.'+algo+method_combination\n",
    "    test_results = spark.sql(s)\n",
    "\n",
    "    #Udf function for calling the python function in pyspark code \n",
    "    oracle_prediction = udf(oracle, IntegerType())\n",
    "\n",
    "    #call udf wrap function\n",
    "    oracle_df = test_results.withColumn('orc_predict',oracle_prediction(col(\"label\"), col(\"prediction\"), col(\"list_categ\"))) \n",
    "\n",
    "    pred = []\n",
    "\n",
    "    colle = oracle_df.select(['label', 'orc_predict']).collect()\n",
    "\n",
    "    tru_value = [int(i.label) for i in colle]#Collect the true label in a list \n",
    "    predict_value = [int(i.orc_predict) for i in colle] #Collect the  prediction in a list \n",
    "\n",
    "    print(round(take_f1_scores(tru_value , predict_value ),4)*100)\n",
    "\n",
    "    pred.append(round(take_f1_scores(tru_value , predict_value ),4)*100)\n",
    "\n",
    "\n",
    "    #for  each class\n",
    "    for i in cate:\n",
    "\n",
    "        print( 'Category {} ......'.format(i))\n",
    "\n",
    "        pecific_class = oracle_df.filter((oracle_df.label == i))\n",
    "\n",
    "        labelPredict = pecific_class.select(['label', 'orc_predict']).collect()\n",
    "\n",
    "        tru_value = [int(i.label) for i in labelPredict]#Collect the true label in a list \n",
    "        predict_value = [int(i.orc_predict) for i in labelPredict] #Collect the  prediction in a list\n",
    "\n",
    "        pred.append(round(take_f1_scores(tru_value , predict_value ),4)*100)\n",
    "\n",
    "\n",
    "    per_cla[algo] = pred  \n",
    "\n",
    "    return per_cla\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call Oracle Function  \n",
    "\n",
    "classes_oracle = oracle predictions('_tfidf_simplemajority')  \n",
    "# classes_oracle = oracle predictions('_lda_simplemajority') #UNCOMMENT IF YOU WANT TO FIND THE ORACLE UPPER BOUND FOR LDA MODELLING\n",
    "\n",
    "pd.DataFrame(classes_oracle).set_index('Classes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP-FP Analysis Based on the Average document length and the Average number of sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "#average number of sections\n",
    "doc_numSect = df1.groupby('DOI').agg(count(df1.Section).alias('number_sections'))\n",
    "\n",
    "\n",
    "\n",
    "int_category = StringIndexer(inputCol='Categories',outputCol='label')\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Full_Document\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "\n",
    "prep_pipeline = Pipeline(stages = [int_category, regexTokenizer, remover])\n",
    "pre_processing = prep_pipeline.fit(df)\n",
    "pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "\n",
    "#create UDF function\n",
    "def document_length(doc_vec):\n",
    "    return int(len(doc_vec))\n",
    "\n",
    "\n",
    "length_f= udf(document_length, IntegerType())\n",
    "\n",
    "final_prediction=pre_processed_data.withColumn('Document_length',length_f(pre_processed_data.filtered))\n",
    "#Average document length\n",
    "doc_length  =final_prediction.select(['DOI','label',\"Document_length\"])\n",
    "\n",
    "#Join both features, namely average length and average number of sections\n",
    "final_features = doc_length.join(doc_numSect , on=['DOI'], how='inner').dropDuplicates().dropna()\n",
    "final_features = final_features.select(['DOI','Document_length', 'number_sections'])\n",
    "display(final_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis TP - FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean,avg\n",
    "\n",
    "cate = [0.0, 1.0, 2.0, 3.0]\n",
    "d = {\"Features\":['TP Avg Length 0', 'TP Avg Sections 0', 'FP Avg Length 0', 'FP Avg Sections 0',\\\n",
    " 'TP Avg Length 1', 'TP Avg Sections 1', 'FP Avg Length 1', 'FP Avg Sections 1',\\\n",
    " 'TP Avg Length 2', 'TP Avg Sections 2', 'FP Avg Length 2', 'FP Avg Sections 2',\\\n",
    " 'TP Avg Length 3', 'TP Avg Sections 3', 'FP Avg Length 3', 'FP Avg Sections 3',\\\n",
    "]\n",
    "    }\n",
    "names = [ \"nb\", 'lr','ovr',\"dtc\",'rfc']\n",
    "# names = [ \"nb\", 'lr',\"dtc\",'rfc']\n",
    "\n",
    "def tp_fp_analysis(method_combination):\n",
    "  '''\n",
    "  Analysis of ture positives false positives\n",
    "  based on the average document length and\n",
    "  average number of sections\n",
    "  '''\n",
    "\n",
    "\n",
    "\n",
    "  for alg in names:\n",
    "    print(str(alg))\n",
    "    feat = []\n",
    "    \n",
    "    #SQL query for retreiving the saved tables in Databricks\n",
    "    s = 'SELECT * FROM taxiarchis.'+ alg + method_combination\n",
    "    test_results = spark.sql(s)\n",
    "    test = test_results.select(['DOI', 'label','prediction'])\n",
    "\n",
    "\n",
    "    for i in cate:\n",
    "        print( 'Category {} ......'.format(i))\n",
    "\n",
    "        TP = test.filter((test.label == i) & (test.prediction == i))\n",
    "        FP = test.filter((test.prediction == i) & (test.label != i))\n",
    "\n",
    "        #True Positives\n",
    "        Tp_t = final_features.join(TP.select('DOI') , on=['DOI'], how='inner').dropDuplicates().dropna()\n",
    "\n",
    "        #False Positives\n",
    "        Tp_f = final_features.join(FP.select('DOI') , on=['DOI'], how='inner').dropDuplicates().dropna()\n",
    "\n",
    "        # True Positives\n",
    "        try:\n",
    "        feat.append( int(Tp_t.select(avg(col('Document_length'))).collect()[0][0])) # Length text TP\n",
    "        except:\n",
    "        feat.append(float('nan'))\n",
    "\n",
    "        try:\n",
    "        feat.append(  int(Tp_t.select(avg(col('number_sections'))).collect()[0][0])) # Number of sections TP\n",
    "        except:\n",
    "        feat.append(float('nan'))\n",
    "\n",
    "        #False Positives\n",
    "        try:\n",
    "        feat.append( int(Tp_f.select(avg(col('Document_length'))).collect()[0][0]))# Length text FP\n",
    "        except:\n",
    "        feat.append(float('nan'))\n",
    "\n",
    "        try:\n",
    "        feat.append(  int(Tp_f.select(avg(col('number_sections'))).collect()[0][0]))# Number of sections FP\n",
    "        except:\n",
    "        feat.append(float('nan'))\n",
    "\n",
    "    #Append dictionary\n",
    "    d[alg] = feat\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_fn = tp_fp_analysis('_tfidf_full')\n",
    "\n",
    "#UNCOMMENT ANY OF THE FOLLOWING TO TEST THE REST OF THE METHODS AND TEXT MODELLINGS\n",
    "# tp_fn = tp_fp_analysis('_lda_full')\n",
    "# tp_fn = tp_fp_analysis('_tfidf_simplemajority')\n",
    "# tp_fn = tp_fp_analysis('_lda_simplecmajority')\n",
    "# tp_fn = tp_fp_analysis('_tfidf_probabilisticmajority')\n",
    "# tp_fn = tp_fp_analysis('_lda_probabilisticmajority')\n",
    "\n",
    "pd.DataFrame(classes_oracle).set_index('Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common predicted documents (UNION) between the methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "def find_common_docs(first_method, second_method, simpleMajority =True):\n",
    "    '''\n",
    "    Compares two methods by creating a confusion matrix with the common\n",
    "    TP, FP, FN for each algorithms in the compared methods.\n",
    "\n",
    "    simpleMajority: Set False if one of the methods is the probabilistic majority,\n",
    "                  else by default is True.\n",
    "    '''\n",
    "\n",
    "    comparison_matices = {}\n",
    "\n",
    "    if simpleMajority:\n",
    "    names = [ \"nb\", 'lr','ovr',\"dtc\",'rfc']\n",
    "    else:\n",
    "    names = [ \"nb\", 'lr',\"dtc\",'rfc']\n",
    "\n",
    "    for algo in names:\n",
    "        print(str(algo))\n",
    "\n",
    "        #Compared methods - SQL queries for retreiving the saved tables in Databricks\n",
    "        first_representation = 'SELECT * FROM taxiarchis.'+ algo+ first_method\n",
    "        second_representation = 'SELECT * FROM taxiarchis.'+ algo+ second_method\n",
    "\n",
    "        first_df = spark.sql(first_representation)\n",
    "        second_df = spark.sql(second_representation)\n",
    "\n",
    "\n",
    "\n",
    "        per_cla = {'Classes':['0','1','2','3'] \n",
    "        }\n",
    "\n",
    "        cate = [0.0, 1.0, 2.0, 3.0]\n",
    "        cate2 = [0.0, 1.0, 2.0, 3.0]\n",
    "\n",
    "        for i in cate:\n",
    "            print(i)\n",
    "            temp = []\n",
    "            for j in cate2:\n",
    "\n",
    "\n",
    "                first_filtered_df = first_df.filter((first_df.label == j) & (first_df.prediction == i))\n",
    "                second_filtered_df =  second_df.filter(( second_df.label == j) & ( second_df.prediction == i))\n",
    "\n",
    "                #Union- find the common documents of the predictions based on the common DOIs\n",
    "                compare_df = first_filtered_df.join(second_filtered_df.select(['DOI']) , on=['DOI'], how='inner').dropDuplicates().dropna()\n",
    "                temp.append(compare_df.count())\n",
    "\n",
    "            per_cla[str(i)] = temp\n",
    "\n",
    "        #Append for each algorithm the matrix\n",
    "        dtaframe = pd.DataFrame(per_cla).set_index('Classes')\n",
    "\n",
    "        comparison_matices[algo] =  dtaframe\n",
    "    \n",
    "    return comparison_matices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_confMatrix =  find_common_docs('_tfidf_full', '_tfidf_simplemajority')\n",
    "\n",
    "#UNCOMMENT IF YOU WANT TO TEST MORE COMPARISONS\n",
    "\n",
    "# union_confMatrix =  find_common_docs('_lda_full', '_lda_simplemajority') \n",
    "# union_confMatrix =  find_common_docs('_tfidf_full', '_tfidf_probabilisticmajority',False)\n",
    "# union_confMatrix =  find_common_docs('_lda_full', '_lda_probabilisticmajority',False)\n",
    "# union_confMatrix =  find_common_docs('_lda_simplemajority', '_lda_probabilisticmajority', False)                                     \n",
    "# union_confMatrix =  find_common_docs('_tfidf_simplemajority', '_tfidf_probabilisticmajority',False)   \n",
    "\n",
    "\n",
    "dict_algo={'nb':'Multinomial Naive Bayes',\n",
    "           'lr':'Multinomial Logistic Regression',\n",
    "           'dtc':'Decision Tree Classifier',\n",
    "           'rfc':'Random Forest Classifier',\n",
    "           'ovr':'One vs Rest Linear SVM'}\n",
    "\n",
    "#Print all the matrices in latex format\n",
    "for i  in  dict_algo.keys():\n",
    "    print(str(dict_algo[i]))\n",
    "    print(comparison_matices[i].to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "All_Analysis",
  "notebookId": 1041476
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
