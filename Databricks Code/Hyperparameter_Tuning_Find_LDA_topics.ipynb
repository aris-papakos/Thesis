{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (RegexTokenizer, Tokenizer, HashingTF, IDF,\n",
    "                                StopWordsRemover, CountVectorizer, StopWordsRemover, StringIndexer, OneHotEncoder)\n",
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n",
    "                                  MulticlassClassificationEvaluator)\n",
    "from pyspark.sql.types import (LongType ,StringType, IntegerType,\n",
    "                               FloatType, DoubleType, ArrayType)\n",
    "from pyspark.sql.functions import col, udf, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DOIs and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load 4 Categories Mapping\n",
    "df_doiCateg = spark.sql(\"SELECT * FROM taxiarchis.doi_categories_4\")\n",
    "\n",
    "# df_doiCateg.count()\n",
    "# indexer = StringIndexer(inputCol=\"Category\", outputCol=\"label\")\n",
    "indexer = StringIndexer(inputCol=\"Categories\", outputCol=\"label\")\n",
    "indexed = indexer.fit(df_doiCateg).transform(df_doiCateg)\n",
    "\n",
    "#transfrom to pandas\n",
    "df_cat = indexed.toPandas()\n",
    "\n",
    "#Split the DOIs for training and Test-----randomstate =42\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_cat['DOI'],df_cat['label'] , test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Document\n",
    "# df = spark.sql(\"SELECT * FROM taxiarchis.fulldocument_4\")\n",
    "# df = df.dropna()\n",
    "\n",
    "#Per Section Documents\n",
    "df = spark.sql(\"SELECT * FROM taxiarchis.persection_4_1\")\n",
    "df = df.dropna()\n",
    "\n",
    "df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fullDocument(df):\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Full_Document\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='count_vec')\n",
    "    idf = IDF(inputCol='count_vec', outputCol='features')\n",
    "\n",
    "    #STRINGINDEXER THE DOC_ID\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Categories',outputCol='label')\n",
    "    int_sections = StringIndexer(inputCol = 'DOI', outputCol ='uni_id')\n",
    "\n",
    "\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_sections, regexTokenizer, remover, count_vec, idf])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    return pre_processed_data.select(['Categories','uni_id','DOI','label','features'])\n",
    "\n",
    "\n",
    "# final_data = tfidf_fullDocument(df)\n",
    "# final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tfidf_section(df):\n",
    "    '''\n",
    "    Vectorize sections in tf-idf text\n",
    "    Representation.\n",
    "    '''\n",
    "\n",
    "    #FOR THE FULL DOCUMENT NEED TO BE CHANGED!!\n",
    "\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Section\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='count_vec')\n",
    "    idf = IDF(inputCol='count_vec', outputCol='features',minDocFreq=2)\n",
    "\n",
    "    #STRINGINDEXER THE DOC_ID\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "    int_sections = StringIndexer(inputCol = 'doc_id', outputCol ='uni_sec_id')\n",
    "\n",
    "\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_sections, regexTokenizer, remover, count_vec, idf])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    return pre_processed_data.select(['doc_id','uni_sec_id','DOI','label','features']).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_fullDocument(df,numTopics = 30):\n",
    "    '''\n",
    "    Vectorize full document's text by representing them \n",
    "    with lda features. The function returns a dataframe.\n",
    "    '''\n",
    "    int_category = StringIndexer(inputCol='Categories',outputCol='label')\n",
    "    int_id = StringIndexer(inputCol='DOI',outputCol='id') #map each unique document with an id\n",
    "    regexTokenizer = RegexTokenizer(inputCol=\"Full_Document\", outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='features')\n",
    "\n",
    "    #PIPELINE\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_id, regexTokenizer, remover, count_vec])\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    final_data_LDA = pre_processing.transform(df).cache()\n",
    "\n",
    "    final_data_LDA = final_data_LDA.dropDuplicates(['id'])\n",
    "    final_data_LDA = final_data_LDA.select(['DOI','id','features','Full_Document','Categories','label'])\n",
    "\n",
    "\n",
    "    # Trains a LDA model.\n",
    "    lda = LDA(k=numTopics, maxIter=50, optimizer=\"em\")\n",
    "    model = lda.fit(final_data_LDA.select('id','features'))\n",
    "\n",
    "\n",
    "    #Create LDA features\n",
    "    transformed = model.transform(final_data_LDA)\n",
    "    return transformed.select(['DOI','label','topicDistribution']).withColumnRenamed(\"topicDistribution\", \"features\").cache()\n",
    "\n",
    "final_data = LDA_fullDocument(df)\n",
    "final_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA - Section Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_representation_sections(df,numTopics = 30):\n",
    "    '''\n",
    "    Vectorize documents of sections by representing them \n",
    "    in lda text representation. The function returns a dataframe.\n",
    "    '''\n",
    "\n",
    "    #FOR THE FULL DOCUMENT NEED TO BE CHANGED!!\n",
    "    regexTokenizer = RegexTokenizer(inputCol='Section', outputCol=\"tokens\", pattern=\"\\\\W\")\n",
    "    remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "    count_vec = CountVectorizer(inputCol='filtered', outputCol='features')\n",
    "\n",
    "    int_category = StringIndexer(inputCol='Category',outputCol='label')\n",
    "    int_id = StringIndexer(inputCol='doc_id',outputCol='id') #map each unique document with an id\n",
    "    #PIPELINE\n",
    "\n",
    "\n",
    "    prep_pipeline = Pipeline(stages = [int_category, int_id, regexTokenizer, remover, count_vec])\n",
    "\n",
    "    pre_processing = prep_pipeline.fit(df)\n",
    "    pre_processed_data = pre_processing.transform(df)\n",
    "\n",
    "    final_data_LDA = pre_processed_data.cache()\n",
    "\n",
    "    final_data_LDA = final_data_LDA.dropDuplicates(['id'])\n",
    "    # final_data_LDA = final_data_LDA.select(['id','DOI','contENT','features','Section','Category','Sections','label'])\n",
    "\n",
    "    # Trains a LDA model.\n",
    "    lda = LDA(k=numTopics, maxIter=50, optimizer=\"em\")\n",
    "    model = lda.fit(final_data_LDA.select('id','features'))\n",
    "\n",
    "\n",
    "    #Create LDA features\n",
    "    transformed = model.transform(final_data_LDA)\n",
    "    return transformed.select(['DOI','label','topicDistribution']).withColumnRenamed(\"topicDistribution\", \"features\").cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_measures(new_df_p):\n",
    "    '''\n",
    "    Predict all the evaluation Measures:\n",
    "    Accuracy, F1-Score, Recall, Precision\n",
    "    '''\n",
    "    \n",
    "    ev_m = []\n",
    "    \n",
    "    #ACCURACY\n",
    "    acc_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName = 'accuracy')\n",
    "    accuracy = acc_eval.evaluate(new_df_p)\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    ev_m.append(accuracy)\n",
    "    \n",
    "    #F1-SCORE\n",
    "    acc_eval_f1 = MulticlassClassificationEvaluator(metricName = 'f1')\n",
    "    f1_score = acc_eval_f1.evaluate(new_df_p)\n",
    "    print(\"f1 = %g\" % f1_score)\n",
    "    ev_m.append(f1_score)\n",
    "    \n",
    "    #RECALL\n",
    "    acc_eval_recall = MulticlassClassificationEvaluator(metricName = 'weightedRecall')\n",
    "    recall = acc_eval_recall.evaluate(new_df_p)\n",
    "    print(\"weightedRecall = %g\" % recall)\n",
    "    ev_m.append(recall)\n",
    "    \n",
    "    #PRECISION\n",
    "    acc_eval_precission = MulticlassClassificationEvaluator(metricName = \"weightedPrecision\")\n",
    "    precission = acc_eval_precission.evaluate(new_df_p)\n",
    "    print(\"weightedPrecision = %g\" % precission)\n",
    "    ev_m.append(precission)\n",
    "    \n",
    "    return ev_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms Hyper-parameters Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.classification import (RandomForestClassifier, GBTClassifier,\n",
    "                                      DecisionTreeClassifier)\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#Multinomial Naive Bayes\n",
    "nb = NaiveBayes()\n",
    "\n",
    "#Decision Tree - Random Forest \n",
    "dtc = DecisionTreeClassifier(maxDepth = 12)\n",
    "# maxDepth = 15\n",
    "rfc = RandomForestClassifier(numTrees = 200, maxDepth = 18)\n",
    "\n",
    "#Linear SVM - One vs All\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.2)\n",
    "ovr = OneVsRest(classifier=lsvc  )\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(maxIter=15, regParam=0.0, elasticNetParam=0.0 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training - Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(final_data):\n",
    "    #train\n",
    "    train = final_data.filter(final_data.DOI.isin(list(X_train)))\n",
    "\n",
    "    #test\n",
    "    test = final_data.filter(final_data.DOI.isin(list(X_test)))\n",
    "    return (train, test)\n",
    "\n",
    "# train, test = split_train_test(final_data)\n",
    "# print(train.count())\n",
    "# print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Vote Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from pyspark.sql.functions import udf, max\n",
    "from pyspark.sql.types import LongType,StringType,IntegerType, FloatType,DoubleType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "def find_maximum(test_results):\n",
    "    '''\n",
    "    Finds the maximum probability and \n",
    "    prepares the dataframe for the Majority Vote.\n",
    "    '''\n",
    "\n",
    "    udf_wf_var = udf(lambda x: round(np.max(np.array(x)),10), returnType=FloatType()) #Define UDF function\n",
    "    df =  test_results.withColumn('WF_Var',udf_wf_var('probability')).dropna()\n",
    "\n",
    "    return df.groupby(\"DOI\").agg(F.collect_list('prediction').alias('list_categ'),F.collect_list('WF_Var').alias('prob_categ'))\n",
    "\n",
    "\n",
    "def majority_vote_percent(pre,prob):\n",
    "    '''\n",
    "    Predict the majority vote based on the given \n",
    "    probabilities  of the classifier by\n",
    "    returning randomly a class when we have more\n",
    "    than one winner.\n",
    "    '''\n",
    "    \n",
    "    maj_vote ={}\n",
    "\n",
    "    for prdi, prba in zip(pre,prob):\n",
    "        \n",
    "        \n",
    "        if prdi in maj_vote:\n",
    "            maj_vote[prdi] += prba    \n",
    "        else:\n",
    "            maj_vote[prdi] = prba\n",
    "        \n",
    "    winners = []\n",
    "    max_count = np.max(maj_vote.values())  \n",
    "    for vote, count in maj_vote.items():\n",
    "        if count == max_count:\n",
    "            winners.append(vote)\n",
    "    \n",
    "    return random.choice(winners)\n",
    "  \n",
    "def max_probability_class(pre,prob):\n",
    "    '''\n",
    "    Returns the class with the maximum \n",
    "    section probability \n",
    "    '''\n",
    "    max_pro = 0.0\n",
    "    max_class = 0\n",
    "    for prdi, prba in zip(pre,prob):\n",
    "    if prba > max_pro:\n",
    "        max_pro = prba\n",
    "        max_class = prdi\n",
    "\n",
    "    return max_class\n",
    "  \n",
    "    \n",
    "def majority_final_prediction(test_results):\n",
    "    '''\n",
    "    Makes the final prediction based on the majority vote\n",
    "    results of probabilities.\n",
    "    '''\n",
    "    #Find Maximum Probability and prepare dataframe for majority vote\n",
    "    df_for_majority_vote = find_maximum(test_results)\n",
    "\n",
    "    #create UDF function\n",
    "    majority_function_probab = udf(majority_vote_percent, DoubleType())\n",
    "\n",
    "    final_prediction=df_for_majority_vote.withColumn('prediction',majority_function_probab(df_for_majority_vote.list_categ, df_for_majority_vote.prob_categ))\n",
    "\n",
    "    # Inner Merge with real Categories\n",
    "    return final_prediction.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna().cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import random \n",
    "from pyspark.sql.types import IntegerType,DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "def prepare_rand_majority(df):\n",
    "    '''\n",
    "    Return a dataframe which contains\n",
    "    in each row a document and a list of\n",
    "    the predictions of the classifier prepared \n",
    "    for the majority vote.\n",
    "    '''\n",
    "    return  df.groupby(\"DOI\").agg(F.collect_list(\"prediction\").alias('list_categ'))\n",
    "\n",
    "\n",
    "def majority_vote_random(votes):\n",
    "    '''\n",
    "    Predict the majority vote by\n",
    "    returning randomly when we have more\n",
    "    than one winner class.\n",
    "    '''\n",
    "\n",
    "    vote_counts = {}\n",
    "    \n",
    "    for vote in votes:\n",
    "        if vote in vote_counts:\n",
    "            vote_counts[vote] += 1\n",
    "        else:\n",
    "            vote_counts[vote] = 1\n",
    "    winners = []\n",
    "    max_count = np.max(vote_counts.values())\n",
    "    \n",
    "    for vote, count in vote_counts.items():\n",
    "        if count == max_count:\n",
    "            winners.append(vote)\n",
    "            \n",
    "    return random.choice(winners)\n",
    "\n",
    "  \n",
    "def final_prediction_maj_random(df):\n",
    "    '''\n",
    "    Computes the final predictions based\n",
    "    on a simple majority vote.\n",
    "    '''\n",
    "    df_for_majority_vote = prepare_rand_majority(df)\n",
    "    #udf apply function\n",
    "    majority_function = udf(lambda z: majority_vote_random(z), DoubleType())\n",
    "    f_res = df_for_majority_vote.withColumn('prediction', majority_function(col('list_categ')))\n",
    "    return f_res.join(indexed , on=['DOI'], how='inner').dropDuplicates().dropna().cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms - Cross Validation - Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_eval = MulticlassClassificationEvaluator(metricName = 'accuracy')\n",
    "\n",
    "paramGrid_lr = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0, 0.5,0.1,0.15, 0.2, 0.3]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 0.1,0.15, 0.2, 0.3]) # Elastic Net Parameter (Ridge = 0)\n",
    "#              .addGrid(lr.maxIter, [10, 20, 25]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 3k-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid_lr, \\\n",
    "                    evaluator=acc_eval, \\\n",
    "                    numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainingSummary = paper_class.bestModel\n",
    "print 'Best Param (regParam): ', trainingSummary._java_obj.getRegParam()\n",
    "print 'Best Param (MaxIter): ', trainingSummary._java_obj.getMaxIter()\n",
    "print 'Best Param (elasticNetParam): ', trainingSummary._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Linear SVM cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(maxIter=15, regParam=0.15)\n",
    "\n",
    "acc_eval = MulticlassClassificationEvaluator(metricName = 'accuracy')\n",
    "\n",
    "paramGrid_lsvc = (ParamGridBuilder()\n",
    "             .addGrid(lsvc.regParam, [0.0, 0.5,0.1,0.15, 0.2, 0.25, 0.3,0.4]) # regularization parameter\n",
    "             .addGrid(lsvc.maxIter, [10,15, 20, 25]) #Number of iterations\n",
    "             .build())\n",
    "\n",
    "# Create 3k-fold CrossValidator\n",
    "cv_lsvc = CrossValidator(estimator=lsvc, \\\n",
    "                    estimatorParamMaps=paramGrid_lsvc, \\\n",
    "                    evaluator=acc_eval, \\\n",
    "                    numFolds=2)\n",
    "\n",
    "paper_class_lsvc = cv_lsvc.fit(train)\n",
    "test_results = paper_class_lsvc.transform(test)\n",
    "\n",
    "new_df = final_prediction_maj_random(test_results).cache() \n",
    "pre_eval_ms= evaluation_measures(new_df)\n",
    "print(pre_eval_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = paper_class_lsvc.bestModel\n",
    "print 'Best Param (regParam): ', trainingSummary._java_obj.getRegParam()\n",
    "print 'Best Param (MaxIter): ', trainingSummary._java_obj.getMaxIter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation For Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator(metricName = 'accuracy')\n",
    "\n",
    "paramGrid_dtc = (ParamGridBuilder()\n",
    "             .addGrid(dtc.maxDepth , [5,10,12,14,15,20,25]) # max tree depth\n",
    "             .build())\n",
    "\n",
    "# Create 2 -fold CrossValidator\n",
    "cv_dtc = CrossValidator(estimator=dtc, \\\n",
    "                    estimatorParamMaps=paramGrid_dtc, \\\n",
    "                    evaluator=acc_eval, \\\n",
    "                    numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_train_test(final_data)\n",
    "train = train.coalesce(10).cache()\n",
    "test = test.coalesce(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_class_dtc = cv_dtc.fit(train)\n",
    "test_results = paper_class_dtc.transform(test)\n",
    "new_df = majority_final_prediction(test_results).cache()\n",
    "# new_df = final_prediction_maj_random(test_results).cache() \n",
    "pre_eval_ms= evaluation_measures(new_df)\n",
    "print(pre_eval_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = paper_class_dtc.bestModel\n",
    "print 'Best Param (maxDepth): ', trainingSummary._java_obj.getMaxDepth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation For Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator(metricName = 'accuracy')\n",
    "rfc = RandomForestClassifier(numTrees = 50, maxDepth =20)\n",
    "paramGrid_rfc = (ParamGridBuilder()\n",
    "#              .addGrid(rfc.maxDepth , [10,12,13,15,20])\n",
    "             .addGrid(rfc.numTrees , [200,220,240])\n",
    "             .build())\n",
    "\n",
    "# Create 2k-fold CrossValidator\n",
    "cv_rfc = CrossValidator(estimator=rfc, \\\n",
    "                    estimatorParamMaps=paramGrid_rfc, \\\n",
    "                    evaluator=acc_eval, \\\n",
    "                    numFolds=2)\n",
    "\n",
    "paper_class_rfc = cv_rfc.fit(train)\n",
    "test_results = paper_class_rfc.transform(test)\n",
    "#   new_df = final_prediction_maj_random(test_results).cache()  #for simple majority vote\n",
    "\n",
    "new_df = majority_final_prediction(test_results).cache()    #for majority vote probabilities\n",
    "pre_eval_ms= evaluation_measures(new_df)\n",
    "myFormattedList = [ '%.4f' % elem for elem in pre_eval_ms ]\n",
    "# measures[n] = myFormattedList\n",
    "print(pre_eval_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = paper_class_rfc.bestModel\n",
    "# print 'Best Param (maxDepth): ', trainingSummary._java_obj.getMaxDepth()\n",
    "print 'Best Param (numTrees): ', trainingSummary._java_obj.getNumTrees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA tuning - Number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_algo={'nb':'Mult. Naive Bayes',\n",
    "           'lr':'Mult. Logistic Regression',\n",
    "           'dtc':'Decision Tree',\n",
    "           'rfc':'Random Forest',\n",
    "           'ovr':'One vs Rest Linear SVM'}\n",
    "accuracy = {'LDA Topics':[],\n",
    "            'Mult. Naive Bayes':[],\n",
    "            'One vs Rest Linear SVM':[],\n",
    "            'Mult. Logistic Regression':[],\n",
    "            'Decision Tree':[],\n",
    "            'Random Forest':[]    \n",
    "}\n",
    "\n",
    "n_topics = [20,30,50,70,100]\n",
    "final_l = []\n",
    "\n",
    "algorithms = [nb,dtc, rfc,lr,ovr]\n",
    "names = [ \"nb\",\"dtc\",'rfc', 'lr','ovr']\n",
    "\n",
    "for topic in n_topics:\n",
    "    accuracy['LDA Topics'].append(topic) # append with number of topics\n",
    "\n",
    "    #train LDA for features production\n",
    "    print('Train the LDA Algorithm.....')\n",
    "    print('Train the LDA Algorithm for {} topics'.format(topic))\n",
    "    final_data = lda_representation_sections(df,numTopics = topic)\n",
    "    print('LDA Algorithm Trained')\n",
    "    #Split tarin test set \n",
    "    train, test = split_train_test(final_data)\n",
    "    train = train.cache()\n",
    "    test = test.cache()\n",
    "\n",
    "    measures = {'Measures':['Accuracy','F1-Score','Recall', 'Precision']}\n",
    "\n",
    "    for algo, n in zip(algorithms,names):\n",
    "    \n",
    "        print(dict_algo[n])\n",
    "        paper_class_2 = algo.fit(train)\n",
    "        test_results = paper_class_2.transform(test)\n",
    "        new_df = final_prediction_maj_random(test_results).cache() \n",
    "        #     new_df = majority_final_prediction(test_results).cache()    \n",
    "        pre_eval_ms= evaluation_measures(new_df)\n",
    "        myFormattedList = [ '%.4f' % elem for elem in pre_eval_ms ]\n",
    "        measures[n] = myFormattedList\n",
    "        print(pre_eval_ms)\n",
    "    \n",
    "    accuracy[dict_algo[n]].append(myFormattedList[0]) #append with the accuracy result\n",
    "  #keep the results in a final list\n",
    "  final_l.append(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together- make predictions - Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame(accuracy)\n",
    "df_plot.set_index('LDA Topics')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = df_plot.set_index('LDA Topics').plot(figsize=(10,6), grid=True,style='.-',fontsize =12)\n",
    "ax.set_ylabel(\"Accuracy\",fontsize=14)\n",
    "ax.set_xlabel(\"LDA Topics\",fontsize=14)\n",
    "ax.legend(loc='right')\n",
    "display(ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "name": "Find_LDA_topics",
  "notebookId": 1007132
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
